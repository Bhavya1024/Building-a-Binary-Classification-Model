{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0bbc62-2502-43c0-ad7a-9495fcbaa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import class_likelihood_ratios\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f279737-6cd0-4dc6-a2c3-63d6f81a4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read emoticon dataset\n",
    "train_emoticon_df = pd.read_csv(\"datasets/train/train_emoticon.csv\")\n",
    "train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
    "train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
    "\n",
    "# read emoticon validation-set\n",
    "test_emoticon_df = pd.read_csv(\"datasets/valid/valid_emoticon.csv\")\n",
    "test_emoticon_X = test_emoticon_df['input_emoticon'].tolist()\n",
    "test_emoticon_Y = test_emoticon_df['label'].tolist()\n",
    "\n",
    "training_data_count = len(train_emoticon_X)\n",
    "validation_data_count = len(test_emoticon_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867e8a27-7d6c-4c5e-9b19-5e857b270e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset in list format (list of [emozi_string, label])\n",
    "DATASET = []\n",
    "for i in range(training_data_count):\n",
    "    DATASET.append([train_emoticon_X[i], train_emoticon_Y[i]])\n",
    "\n",
    "VALIDATION_SET = []\n",
    "for i in range(validation_data_count):\n",
    "    VALIDATION_SET.append([test_emoticon_X[i], test_emoticon_Y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff574f2e-5d1d-4675-bae8-0d3267b2eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return numpy array converting each emozi string to an integer array\n",
    "def to_unicode_decimal_array(emozi_string):\n",
    "    arr = [int(emozi.encode(\"unicode_escape\").hex(),16) for emozi in emozi_string]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d62b25-764d-4842-8153-ee89c2e56fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # size in fraction of complete dataset, dataset randomized if 'randomized == True'\n",
    "def pick_dataset(size, randomized = True, to_int = False):\n",
    "    data_copy = deepcopy(DATASET)\n",
    "    if(randomized == True):\n",
    "        np.random.shuffle(data_copy)\n",
    "    req_split = data_copy[:int(size*training_data_count)]\n",
    "    if(to_int == True):\n",
    "        mod_split = [[to_unicode_decimal_array(data_point[0]),data_point[1]] for data_point in req_split]\n",
    "    else:\n",
    "        mod_split = [[data_point[0],data_point[1]] for data_point in req_split]\n",
    "    return mod_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4d3bc-ae20-457b-bf76-5211094deb33",
   "metadata": {},
   "source": [
    "<h3>Approach 1: convert catagorical data to floating point numbers and train a model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c2ab351-0dc1-4425-ad88-dd545f8ff164",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set = pick_dataset(size=1,randomized=True, to_int=True)\n",
    "test_set = [[to_unicode_decimal_array(data_point[0]),data_point[1]] for data_point in VALIDATION_SET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb1bfd00-f53c-4d20-9db0-5839082a45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training set features and label array in numpy\n",
    "X = []\n",
    "for data in working_set:\n",
    "    X.append(data[0])\n",
    "X = np.array(X)\n",
    "\n",
    "Y = []\n",
    "for data in working_set:\n",
    "    Y.append(data[1])\n",
    "Y = np.array(Y)\n",
    "\n",
    "# creating validation set features and label array in numpy\n",
    "X_test = []\n",
    "for data in test_set:\n",
    "    X_test.append(data[0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "Y_test = []\n",
    "for data in test_set:\n",
    "    Y_test.append(data[1])\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c4f17f2-0385-449f-8675-b9e8d35115a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each feature to a value b/w 0 and !\n",
    "X_normed = (X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0));\n",
    "X_test_normed = (X_test-X_test.min(axis=0))/(X_test.max(axis=0)-X_test.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b0c4f2-3810-4a97-893d-9e7acecbbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF_classifier = svm.SVC(kernel='rbf', gamma='auto', C=1) # SVM with RBF kernel\n",
    "# RBF_classifier.fit(X_normed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9be712f-23ff-4dc3-8b0d-b3ee82485dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP = 0\n",
    "# FP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# for i in range(len(X_normed)):\n",
    "#     prediction = RBF_classifier.predict([X_normed[i]])\n",
    "#     if((prediction == 0) and (Y[i] == 0)):\n",
    "#         TN+=1\n",
    "#     if((prediction == 0) and (Y[i] == 1)):\n",
    "#         FN+=1\n",
    "#     if((prediction == 1) and (Y[i] == 0)):\n",
    "#         FP+=1\n",
    "#     if((prediction == 1) and (Y[i] == 1)):\n",
    "#         TP+=1\n",
    "# train_accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "# print('training accuracy: ',train_accuracy)\n",
    "\n",
    "# TP = 0\n",
    "# FP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# for i in range(len(X_test_normed)):\n",
    "#     prediction = RBF_classifier.predict([X_test_normed[i]])\n",
    "#     if((prediction == 0) and (Y[i] == 0)):\n",
    "#         TN+=1\n",
    "#     if((prediction == 0) and (Y[i] == 1)):\n",
    "#         FN+=1\n",
    "#     if((prediction == 1) and (Y[i] == 0)):\n",
    "#         FP+=1\n",
    "#     if((prediction == 1) and (Y[i] == 1)):\n",
    "#         TP+=1\n",
    "# validation_accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "# print('validation accuracy: ',validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5430a6fd-d109-454b-91b6-05503e6ff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## models that work with floating point features not working well with catagorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826a84a-0b42-4b0c-994b-fac2646263ec",
   "metadata": {},
   "source": [
    "<h3>Approach 2: use Bag of words approach</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4aee2b-d64d-4879-bc5b-00e75ce8b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imagine each emozi is a word having some meaning\n",
    "\n",
    "emozi_set = set()\n",
    "working_set = pick_dataset(1, randomized=True, to_int=False)\n",
    "for data in working_set:\n",
    "    features = data[0]\n",
    "    for emozi in features:\n",
    "        emozi_set.add(emozi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb3a85e0-0996-43aa-9474-544f7e00aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert each datapoint from training set into a sparse vector denoting presence of each emozi\n",
    "\n",
    "converted_dataset = []\n",
    "for datapoint in working_set:\n",
    "    converted_featuereset = np.zeros(len(emozi_set))\n",
    "    feature_list = deepcopy(datapoint[0])\n",
    "    pos = 0\n",
    "    for emozi in emozi_set:\n",
    "        if emozi in feature_list:\n",
    "            converted_featuereset[pos] = 1\n",
    "        pos+=1\n",
    "    converted_dataset.append([converted_featuereset,datapoint[1]])\n",
    "\n",
    "## convert each datapoint from validation set into a sparse vector\n",
    "\n",
    "converted_validation_set = []\n",
    "for datapoint in VALIDATION_SET:\n",
    "    converted_featuereset = np.zeros(len(emozi_set))\n",
    "    feature_list = deepcopy(datapoint[0])\n",
    "    pos = 0\n",
    "    for emozi in emozi_set:\n",
    "        if emozi in feature_list:\n",
    "            converted_featuereset[pos] = 1\n",
    "        pos+=1\n",
    "    converted_validation_set.append([converted_featuereset,datapoint[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eee9603-0c99-44ca-a5c9-933dd0c175e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing training data\n",
    "X = []\n",
    "y = []\n",
    "for data in converted_dataset:\n",
    "    X.append(data[0])\n",
    "    y.append(data[1])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "## preparing validation data\n",
    "X_v = []\n",
    "y_v = []\n",
    "for data in converted_validation_set:\n",
    "    X_v.append(data[0])\n",
    "    y_v.append(data[1])\n",
    "X_v = np.array(X_v)\n",
    "y_v = np.array(y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e269462-f261-4f61-94ef-27f2bcfce5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use:\n",
    "# 1.normal SVM with RBF\n",
    "# 2.Decision Tree\n",
    "# 3.Random Forest\n",
    "# 4.Do all of these with PCA/SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a18675c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45194274028629855"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_classifier = BernoulliNB()\n",
    "NB_classifier.fit(X,y)\n",
    "NB_classifier.score(X_v,y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7255e6b5-451b-4d2b-87fe-7d203a36c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimentionality reduction using SVD\n",
    "# feature lenght of 130 out of total 214 captures 95.3% of the energy of original data\n",
    "x_compressed = csr_matrix(X)\n",
    "x_test_compressed = csr_matrix(X_v)\n",
    "svd = TruncatedSVD(n_components=130)\n",
    "svd.fit(x_compressed)\n",
    "X_reduced = svd.transform(x_compressed)\n",
    "X_test_reduced = svd.transform(x_test_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3410cc3f-bcda-4800-9648-65590889ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7080, 130)\n",
      "(489, 130)\n"
     ]
    }
   ],
   "source": [
    "print(X_reduced.shape)\n",
    "print(X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "508ee6b8-17aa-4811-ba10-2f74d8c7019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decision tree on reduced data\n",
    "# k_val = []\n",
    "# t_acc = []\n",
    "# v_acc = []\n",
    "# for depth in np.arange(1,11,1):\n",
    "#     k_val.append(depth)\n",
    "#     DT_classifier = DecisionTreeClassifier(max_depth=depth)\n",
    "#     DT_classifier.fit(X_reduced,y)\n",
    "#     t_acc.append(DT_classifier.score(X_reduced,y))\n",
    "#     v_acc.append(DT_classifier.score(X_test_reduced,y_v))\n",
    "\n",
    "# plt.plot(k_val, t_acc)\n",
    "# plt.plot(k_val, v_acc)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61350517-7512-4867-b55e-67f6faac24f8",
   "metadata": {},
   "source": [
    "<h3>Approach 3: Creating encoding using deep learning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717cb75-6130-4c9a-b97f-9af66968f81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c85e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d106b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1248285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6cf1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956b2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38cca6-9bb8-4437-837c-6f08c2f217b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc0cc1-8558-40b2-93e3-fa5734a93fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7594ae-72f9-48bd-ac8b-06a1628a07c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fdbbf30-27c4-47db-adb3-204b8392b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[0 1 1 ... 1 1 0]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0\n",
      " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
      " 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
      " 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0\n",
      " 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
      " 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)\n",
    "print(X_v)\n",
    "print(y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e590975-8bb3-404a-9f2b-cbe8a3bdb319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😛🛐😻😑😣🙠🙯🚼😒🙼😑🙯😣', 0]\n",
      "['🛐😑😪😛🚼🙯😣🚅😑🙯😹😣🙼', 0]\n",
      "['😛🙯😑🚡😣🚼🛐🙲😣🙯🛑😑🙼', 0]\n",
      "['😛🚼🛐🙐😣🙯😑🙪😑🙼🛆😣🙯', 1]\n",
      "['🛐🚟🚼😛🙋😑😣🙯😹🙯😑😣🙼', 1]\n",
      "['😑😣🚧😛🚜🚼🙯🛐🙼😣😑🙕🙯', 1]\n",
      "['😣😑🙯🚼🛐🚥😬😛😣🚄😑🙼🙯', 0]\n",
      "['🚡🚼😑🛐🚔🙯😛😣😑🙯🛓🙼😣', 0]\n",
      "['🛐😛🛜😑🚼😚😣🙯😣😑🙯🚠🙼', 0]\n",
      "['🙯😑🙷🛐🚼😣😛😍😿🙯🙼😑😣', 1]\n",
      "['😣🙯🛐😑😛🚼🙚😍🙯🙼😣😑😸', 0]\n",
      "['😛😯🚼🙯😑🛐😻😣🙼🙯😹😑😣', 1]\n",
      "['😑🙯😛🛐🚼🙒🚙😣🙯😣😑🙼😬', 1]\n",
      "['😿😣🚼🚴😛😑🙯🛐😣🙯😑😴🙼', 1]\n",
      "['😛🛐🚼🚂🙯😦😣😑🙯😑😣🙼🙨', 1]\n",
      "['🛐🚼🙯🛝😑😉😛😣🙒😣🙼😑🙯', 0]\n",
      "['😣🙯😵😑😛🚃🚼🛐😣🙯🚜🙼😑', 0]\n",
      "['😛😑🙯🚼🛆🛐😘😣😣😊🙼😑🙯', 0]\n",
      "['🚟😣😑🛐😌🚼😛🙯🙯😣😑😿🙼', 1]\n",
      "['😛🚼😑😣🙧🛐🙯😉🚍😣😑🙯🙼', 0]\n",
      "['🛓🛐😣🚼🙯🚥😑😛😢😣🙯😑🙼', 1]\n",
      "['😣😑🚵😛🚼🛐🙯😊🙼😣🙯😑😩', 0]\n",
      "['🛐🙯😛🚼😣😬😠😑🙯🙼😣😑🛡', 0]\n",
      "['🛐🙯🚜😣🚼😑😰😛😑🙯🚏🙼😣', 1]\n",
      "['🚼😛🙯😣🚗🛐🙸😑😣🙼🙯🚌😑', 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print([train_emoticon_X[i],train_emoticon_Y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6816140-1e41-47bc-92e8-eac7a95c8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_emoticon_X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eacefd45-1ace-4433-a47d-713143a8cc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436029161998574100559715"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(train_emoticon_X[0][9].encode(\"unicode_escape\").hex(),16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ac1909c-7776-44ea-8888-eda564bd32b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[436029161998574100558178, 436029161998574100571184, 436029161998574100558690, 436029161998574100558129, 436029161998574100558387, 436029161998574100559408, 436029161998574100559462, 436029161998574100570723, 436029161998574100558130, 436029161998574100559715, 436029161998574100558129, 436029161998574100559462, 436029161998574100558387]\n"
     ]
    }
   ],
   "source": [
    "arr = to_unicode_decimal_array(train_emoticon_X[0])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32339665-4422-43c8-a15c-706768c05e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "{'🚣', '🚄', '🙱', '😸', '🙄', '\\U0001f6d8', '🙔', '🚆', '🚓', '🚲', '🙯', '🚭', '🚼', '🙊', '😭', '🛆', '🚯', '🙠', '🚃', '🙏', '🙟', '🙅', '🚀', '😒', '😎', '😊', '🙮', '🚾', '🚶', '🙚', '\\U0001f6dc', '🚎', '😲', '🛁', '🙃', '\\U0001f6d9', '🚁', '😣', '😦', '🛏', '🙶', '🚥', '🛅', '🚌', '🚜', '🙩', '🙈', '🛓', '🛎', '😧', '🛍', '🙉', '🚵', '😨', '🛈', '🚕', '🛀', '🙢', '🚡', '😄', '🛕', '🚏', '😴', '😱', '🙗', '😵', '🙸', '\\U0001f6de', '🙾', '🛋', '🙣', '🚟', '🚽', '😽', '🙐', '🛒', '😇', '😅', '😓', '🙫', '😘', '🚹', '😗', '🛇', '😶', '🚉', '🚞', '🚝', '🙳', '🚮', '🛑', '😬', '😛', '🚇', '🙌', '🙑', '🙧', '🙜', '🙬', '🚘', '🚖', '😢', '😥', '😏', '🙍', '🚱', '🚅', '🙇', '🙛', '🙦', '🚷', '😫', '🛂', '🚍', '🚠', '😤', '😿', '🚨', '🙹', '🙼', '🚚', '😀', '🚩', '🙨', '🙕', '🙋', '🙰', '😐', '😺', '😻', '🙻', '🚿', '🙀', '🛡', '🚛', '🚸', '😉', '🛉', '😆', '🚔', '😜', '😯', '😠', '🚈', '🛌', '😩', '😪', '🚂', '😟', '🚑', '😾', '🚙', '🛔', '🙲', '😑', '😮', '🚋', '😃', '🙆', '😂', '🛗', '🙎', '🛊', '🙘', '\\U0001f6dd', '🙙', '🙴', '\\U0001f6db', '😞', '😔', '😰', '😙', '🙁', '🛄', '🚐', '🙞', '😡', '🙷', '😌', '\\U0001f6df', '🛐', '🚗', '🙥', '🚧', '🚢', '🚳', '🛖', '🚒', '🚴', '🚪', '😕', '😍', '🙪', '😖', '🚰', '🙽', '😋', '😷', '😚', '😳', '😁', '😝', '🙤', '🚫', '😼', '🙒', '🚺', '🚦', '🚤', '😹', '🙖', '🙓', '🙿', '🚊'}\n"
     ]
    }
   ],
   "source": [
    "print(len(emozi_set))\n",
    "print(emozi_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952738ba-696c-4510-b122-98a1e3921c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(len(emozi_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f909e4-a7e1-4f5c-a4e4-34392d902c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(converted_dataset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "267f5e0f-13a4-484b-98ef-4276c17480c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "7080\n"
     ]
    }
   ],
   "source": [
    "print(X_v)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8412079c-e599-4648-8400-8fed244717b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054581e-54d3-4906-a11c-c97535e83544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
